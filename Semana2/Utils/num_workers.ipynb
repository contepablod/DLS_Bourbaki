{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish with:1.0734808444976807 seconds, num_workers=0\n",
      "Finish with:5.095129728317261 seconds, num_workers=1\n",
      "Finish with:4.690190076828003 seconds, num_workers=2\n",
      "Finish with:4.923038482666016 seconds, num_workers=3\n",
      "Finish with:5.231919288635254 seconds, num_workers=4\n",
      "Finish with:5.686760187149048 seconds, num_workers=5\n",
      "Finish with:6.780835151672363 seconds, num_workers=6\n",
      "Finish with:8.358283519744873 seconds, num_workers=7\n",
      "Finish with:8.817540168762207 seconds, num_workers=8\n",
      "Finish with:9.85634708404541 seconds, num_workers=9\n",
      "Finish with:10.668752670288086 seconds, num_workers=10\n",
      "Finish with:11.753221035003662 seconds, num_workers=11\n",
      "Finish with:12.922778844833374 seconds, num_workers=12\n"
     ]
    }
   ],
   "source": [
    "# Define a mock dataset\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, num_samples, img_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples (int): Number of samples in the dataset.\n",
    "            img_size (tuple): Image size (C, H, W).\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate a random image (tensor)\n",
    "        image = torch.randn(self.img_size)\n",
    "        # Generate a random \"mask\" or target (for the sake of example)\n",
    "        target = torch.randint(0, 2, (1,))  # Binary target\n",
    "        return image, target\n",
    "\n",
    "# Parameters\n",
    "num_samples = 1024  # Number of samples in the mock dataset\n",
    "img_size = (3, 64, 64)  # Example image size (C, H, W)\n",
    "batch_size = 64\n",
    "\n",
    "# Create the dataset\n",
    "dataset = RandomDataset(num_samples=num_samples, img_size=img_size)\n",
    "\n",
    "cpu_count = os.cpu_count() or 1  # Fallback to 1 if os.cpu_count() returns None\n",
    "\n",
    "for num_workers in range(cpu_count + 1):  # Iterate from 0 to cpu_count\n",
    "    train_dl = DataLoader(dataset,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(10):  # Simulate 4 epochs\n",
    "        for i, data in enumerate(train_dl, 0):\n",
    "            pass  # Here you would process your data\n",
    "    end = time.time()\n",
    "    print(f\"Finish with:{end - start} seconds, num_workers={num_workers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish with:1.0292959213256836 seconds, num_workers=0\n",
      "Finish with:5.832638263702393 seconds, num_workers=1\n",
      "Finish with:5.6890552043914795 seconds, num_workers=2\n",
      "Finish with:5.761323690414429 seconds, num_workers=3\n",
      "Finish with:5.874486684799194 seconds, num_workers=4\n",
      "Finish with:6.3858232498168945 seconds, num_workers=5\n",
      "Finish with:7.5590221881866455 seconds, num_workers=6\n",
      "Finish with:8.2344229221344 seconds, num_workers=7\n",
      "Finish with:8.8154878616333 seconds, num_workers=8\n",
      "Finish with:9.214709758758545 seconds, num_workers=9\n",
      "Finish with:9.995859861373901 seconds, num_workers=10\n",
      "Finish with:10.611126184463501 seconds, num_workers=11\n",
      "Finish with:12.081108093261719 seconds, num_workers=12\n"
     ]
    }
   ],
   "source": [
    "for num_workers in range(cpu_count + 1):  # Iterate from 0 to cpu_count\n",
    "    train_dl = DataLoader(dataset,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=False,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(10):  # Simulate 4 epochs\n",
    "        for i, data in enumerate(train_dl, 0):\n",
    "            pass  # Here you would process your data\n",
    "    end = time.time()\n",
    "    print(f\"Finish with:{end - start} seconds, num_workers={num_workers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate a counterintuitive trend: as the number of workers increases, the time to process the data also increases, rather than decreasing. This is unusual because the expectation is that adding more workers would parallelize the data loading and potentially decrease the total processing time, up to a certain point, depending on the task's I/O and CPU-bound characteristics.\n",
    "\n",
    "However, in this case, the increase in processing time with more workers suggests a few possibilities:\n",
    "\n",
    "    Overhead from Too Many Workers: If the task of loading and preprocessing the data is relatively lightweight (as it might be with generating random data), the overhead of managing multiple worker processes can outweigh the benefits of parallelization. This is particularly true when the dataset is stored in memory (like your mock dataset), and there's minimal I/O latency to hide behind the workers' loading time.\n",
    "\n",
    "    Contention: With too many workers, there might be contention for CPU resources, especially if the number of workers significantly exceeds the number of available CPU cores. This can lead to context switching overhead and reduced efficiency.\n",
    "\n",
    "    Implementation Details: Depending on how PyTorch and the DataLoader are implemented and interact with the system's threading and multiprocessing capabilities, there could be inefficiencies that become more pronounced with a higher number of workers.\n",
    "\n",
    "    Synchronization Overhead: When all the worker processes try to return their batches of data back to the main process, there might be a synchronization overhead, especially if the main process is not able to consume the data as quickly as it is produced.\n",
    "\n",
    "Given these results, it appears that for this specific scenario (using a mock dataset that likely requires minimal disk I/O and possibly minimal preprocessing), having zero or a very low number of worker processes is the most efficient choice. This scenario highlights the importance of tuning the number of workers based on the specific characteristics of the dataset, the complexity of the preprocessing, and the hardware capabilities of the system.\n",
    "\n",
    "For real-world datasets, especially those involving complex preprocessing or significant disk I/O (like loading large images from disk), you might find a different optimal number of workers. It's always a good idea to conduct similar experiments with your actual dataset to find the best configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
