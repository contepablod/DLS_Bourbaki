{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Colegio Bourbaki](./Images/Bourbaki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning & AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos Multimodales de Texto e Imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fue en enero de 2021 cuando **OpenAI** anunció dos nuevos modelos: **DALL-E** y **CLIP**, ambos modelos **multimodales** que conectan **textos e imágenes** de alguna manera. \n",
    "\n",
    "Vamos a implementar el modelo CLIP desde cero en **PyTorch**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP: Modelo Texto-Imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En [Learning Transferable Visual Models From Natural Language Supervision paper](https://arxiv.org/abs/2103.00020), OpenAI presenta su nuevo modelo denominado **CLIP**, por **Contrastive Language-Image Pre-training**. En pocas palabras, este modelo aprende la relación entre una frase completa y la imagen que describe; en el sentido de que cuando el modelo está entrenado, dada una frase de entrada será capaz de recuperar las imágenes más relacionadas correspondientes a esa frase. Lo importante aquí es que se entrena con frases completas en lugar de con clases individuales como coche, perro, etc. La intuición es que cuando se entrena con frases completas, el modelo puede aprender muchas más cosas y encuentra algún patrón entre imágenes y textos.\n",
    "\n",
    "También demuestran que cuando este modelo se entrena con un enorme conjunto de datos de imágenes y sus correspondientes textos, también puede actuar como clasificador. Le animo a estudiar el artículo para saber más sobre este interesante modelo y sus sorprendentes resultados en conjuntos de datos de referencia. Por mencionar sólo uno, ¡el modelo CLIP entrenado con esta estrategia clasifica ImageNet mejor que los modelos SOTA entrenados en la propia ImageNet optimizada para la única tarea de clasificación!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de preparacion, veamos de qué es capaz el modelo final que construiremos. Dada una consulta (texto en bruto) como \"un chico saltando con monopatín\" o \"una chica saltando desde un columpio\", el modelo recuperará las imágenes más relevantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![teaser1](./Images/teaser.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos otra salida:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![teaser2](./Images/dogs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/datasets/adityajn105/flickr8k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.862689,
     "end_time": "2021-04-05T08:01:49.835804",
     "exception": false,
     "start_time": "2021-04-05T08:01:45.973115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import timm\n",
    "\n",
    "# Pretrained Models\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "\n",
    "# Utils\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.autonotebook import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "!set 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:2'\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"__Python VERSION:\", sys.version)\n",
    "print(\"__pyTorch VERSION:\", torch.__version__)\n",
    "print(\n",
    "    \"__CUDA VERSION\",\n",
    ")\n",
    "!nvcc --version\n",
    "print(\"__CUDNN VERSION:\", torch.backends.cudnn.version())\n",
    "print(\"__Number CUDA Devices:\", torch.cuda.device_count())\n",
    "print(\"__Devices\")\n",
    "print(\"Active CUDA Device: GPU\", torch.cuda.current_device())\n",
    "print(\"Available devices \", torch.cuda.device_count())\n",
    "print(\"Current cuda device \", torch.cuda.current_device())\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012732,
     "end_time": "2021-04-05T08:01:51.324144",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.311412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configuraciones del Modelo (setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase de configuración proporcionada (CFG) describe los ajustes del modelo.\n",
    "\n",
    "El uso de modelos preentrenados (resnet50 para imágenes y distilbert-base-uncased para texto) aprovecha el enfoque que aprovecha el aprendizaje por transferencia (Transfer Learning), ajustando modelos que han sido preentrenados en grandes conjuntos de datos para beneficiarse de representaciones aprendidas previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CFG:\n",
    "    debug: bool = False\n",
    "    # Paths\n",
    "    image_path: str = \"./Data/CLIP_Images\"\n",
    "    captions_path: str = \"./Data\"\n",
    "\n",
    "    # Training Parameters\n",
    "    batch_size: int = 16\n",
    "    num_workers: int = 0\n",
    "    epochs: int = 10\n",
    "\n",
    "    # Learning Rates\n",
    "    head_lr: float = 1e-3\n",
    "    image_encoder_lr: float = 1e-4\n",
    "    text_encoder_lr: float = 1e-5\n",
    "    weight_decay: float = 1e-3\n",
    "\n",
    "    # Early Stopping\n",
    "    patience: int = 1\n",
    "    factor: float = 0.8\n",
    "\n",
    "    # Model Configuration\n",
    "    model_name: str = \"resnet50\"\n",
    "    image_embedding: int = 2048\n",
    "    text_encoder_model: str = \"distilbert-base-uncased\"\n",
    "    text_embedding: int = 768\n",
    "    text_tokenizer: str = \"distilbert-base-uncased\"\n",
    "    max_length: int = 200\n",
    "    pretrained: bool = True\n",
    "    trainable: bool = True\n",
    "    temperature: float = 1.0\n",
    "    size: int = 128\n",
    "\n",
    "    # Projection Head Configuration\n",
    "    num_projection_layers: int = 1\n",
    "    projection_dim: int = 256\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Device Configuration\n",
    "    # device: torch.device = field(\n",
    "    #     default_factory=lambda: torch.device(\n",
    "    #         \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    #     )\n",
    "    # )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.377383,
     "end_time": "2021-04-05T08:01:51.714313",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.336930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CFG:\n",
    "#     debug = False\n",
    "#     image_path = \"./Data/Images\"\n",
    "#     captions_path = \"./Data\"\n",
    "#     batch_size = 8 #16\n",
    "#     num_workers = 0\n",
    "#     head_lr = 1e-3\n",
    "#     image_encoder_lr = 1e-4\n",
    "#     text_encoder_lr = 1e-5\n",
    "#     weight_decay = 1e-3\n",
    "#     patience = 1\n",
    "#     factor = 0.8\n",
    "#     epochs = 4\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     model_name = 'resnet50'\n",
    "#     image_embedding = 2048\n",
    "#     text_encoder_model = \"distilbert-base-uncased\"\n",
    "#     text_embedding = 768\n",
    "#     text_tokenizer = \"distilbert-base-uncased\"\n",
    "#     max_length = 200\n",
    "\n",
    "#     pretrained = True # for both image encoder and text encoder\n",
    "#     trainable = True # for both image encoder and text encoder\n",
    "#     temperature = 1.0\n",
    "\n",
    "#     # image size\n",
    "#     size = 128\n",
    "\n",
    "#     # for projection head; used for both image and text encoders\n",
    "#     num_projection_layers = 1\n",
    "#     projection_dim = 256\n",
    "#     dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012746,
     "end_time": "2021-04-05T08:01:51.740070",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.727324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Funciones de ayuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase AvgMeter es una utilidad para el seguimiento de la media, suma y recuento de una métrica particular durante el proceso de entrenamiento o validación del modelo. \n",
    "\n",
    "Este enfoque particularmente útil para realizar un seguimiento de las métricas de pérdida y precisión a lo largo de épocas o lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.023459,
     "end_time": "2021-04-05T08:01:51.776328",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.752869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012817,
     "end_time": "2021-04-05T08:01:51.802043",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.789226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos codificar tanto las imágenes como los textos que describen las imágenes. Por lo tanto, el conjunto de datos debe **devolver tanto imágenes como textos**. \n",
    "\n",
    "Por supuesto, no vamos a introducir texto en bruto en nuestro codificador de texto. Utilizaremos el modelo **DistilBERT** (que es más pequeño que BERT pero funciona casi tan bien como BERT) de la biblioteca **HuggingFace** como codificador de texto; por lo tanto, tenemos que **tokenizar** las frases (pies de foto) con el tokenizador de DistilBERT y luego introducir los identificadores de los tokens (input_ids) y las máscaras de atención (attention_masks) a DistilBERT. \n",
    "\n",
    "Consecuentemente, el conjunto de datos también debe encargarse de la tokenización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo construiremos el dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En **\\_\\_init\\_\\_** recibimos un objeto tokenizer que es en realidad un tokinzer HuggingFace; este tokenizer se cargará al ejecutar el modelo. Estamos rellenando y truncando los subtítulos a un max_length especificado. \n",
    "\n",
    "En **\\_\\_getitem\\_\\_** primero cargaremos un título codificado que es un diccionario con las claves input_ids y attention_mask, haremos tensores con sus valores y después cargaremos la imagen correspondiente, la transformaremos y después la convertiremos en un tensor y la pondremos en el diccionario con \"image\" como clave. Por último, ponemos el texto en bruto del pie de foto con la clave \"pie de foto\" en el diccionario sólo para fines de visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025532,
     "end_time": "2021-04-05T08:01:51.840523",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.814991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and captions must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names\n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)[\"image\"]\n",
    "        item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item[\"caption\"] = self.captions[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012853,
     "end_time": "2021-04-05T08:01:51.866433",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.853580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Codificador de Imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código del codificador de imágenes es sencillo. Usaremos la librería PyTorch Image Models (timm) que pone a nuestra disposición un montón de modelos de imagen diferentes desde ResNets a EfficientNets y muchos más. \n",
    "\n",
    "Aquí vamos a utilizar un ResNet50 como nuestro codificador de imágenes. \n",
    "\n",
    "Podemos usar la librería torchvision para usar ResNets si no quieres instalar una nueva librería."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![resnet](./Images/resnet50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código codifica cada imagen en un vector de tamaño fijo con el tamaño de los canales de salida del modelo (en el caso de ResNet50 el tamaño del vector será **2048**). Esta es la salida después de la capa nn.AdaptiveAvgPool2d()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=CFG.model_name,\n",
    "        pretrained=CFG.pretrained,\n",
    "        trainable=CFG.trainable,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificador de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como mencionamos antes, utilizaremos DistilBERT como codificador de texto. Al igual que su hermano mayor BERT, se añadirán dos tokens especiales a los tokens de entrada: **CLS** y **SEP**, que marcan el inicio y el final de una frase. Para captar la representación completa de una frase (como señalan los documentos relacionados BERT y DistilBERT) utilizamos las representaciones finales del token CLS y esperamos que esta representación capte el significado global de la frase (caption). Pensándolo así, es similar a lo que hicimos con las imágenes y las convertimos en un vector de tamaño fijo.\n",
    "\n",
    "En el caso de DistilBERT (y también de BERT) la representación oculta de salida para cada token es un vector de tamaño **768**. Así pues, todo el subtítulo se codificará en la representación de tokens CLS cuyo tamaño es 768."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![resnet](./Images/distillbert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=CFG.text_encoder_model,\n",
    "        pretrained=CFG.pretrained,\n",
    "        trainable=CFG.trainable,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una capa de **proyección** es un componente utilizado en modelos de aprendizaje automático, especialmente en aprendizaje auto-supervisado y aprendizaje contrastivo. Su propósito principal es transformar las representaciones de características producidas por un modelo (como las salidas de la columna vertebral de una red neuronal) en un espacio donde la tarea subsiguiente (por ejemplo, clasificación, comparación o agrupación) se realiza de manera más efectiva. Esto incluye una o más capas completamente conectadas, y puede incluir capas de normalización y funciones de activación no lineales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos codificado nuestras imágenes y textos en vectores de tamaño fijo (2048 para la imagen y 768 para el texto) necesitamos traerlos (proyectarlos) a un **nuevo espacio** con **dimensiones similares** para ambas imágenes y textos para poder compararlos y apartar la imagen y textos no relevantes y juntar los que coincidan. Así, el siguiente código traerá los vectores de 2048 y 768 dimensiones a un mundo de 256 (projection_dim) dimensiones, donde podremos **compararlos**.\n",
    "\n",
    "\"embedding_dim\" es el tamaño del vector de entrada (2048 para imágenes y 768 para textos) y \"projection_dim\" es el tamaño del vector de salida, que será 256 en nuestro caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_dim, projection_dim=CFG.projection_dim, dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012961,
     "end_time": "2021-04-05T08:01:51.933336",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.920375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Modelo CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí vamos a utilizar los módulos anteriores que hemos construido para implementar el modelo principal. \n",
    "\n",
    "La función **\\_\\_init\\_\\_** se explica por sí misma. En la función forward, primero codificamos las imágenes y los textos por separado en vectores de tamaño fijo (con diferentes dimensionalidades). Después, utilizando módulos de proyección separados, los proyectamos a ese espacio compartido del que hablamos anteriormente. Aquí las codificaciones tendrán una forma similar (256 en nuestro caso). Después calcularemos la pérdida.\n",
    "\n",
    "En **Álgebra Lineal**, una forma común de medir si dos vectores son de características similares (son parecidos entre sí) es calcular su **producto punto** (multiplicando las entradas coincidentes y tomando la suma de ellas); si el número final es grande, son parecidos y si es pequeño ¡no lo son (relativamente hablando)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos hablado de dos vectores, pero, ¿qué tenemos aquí? Tenemos image_embeddings, una matriz con forma (batch_size, 256) y text_embeddings con forma (batch_size, 256). Esto significa que tenemos dos grupos de vectores en lugar de dos vectores individuales. ¿Cómo medimos la similitud entre dos grupos de vectores (dos matrices)? De nuevo, con el producto punto (el operador @ en PyTorch hace el producto punto o multiplicación de matrices en este caso). Para poder multiplicar estas dos matrices entre sí, transponemos la segunda. Bien, obtenemos una matriz con forma (tamaño_lote, tamaño_lote) que llamaremos logits. (la temperatura es igual a 1.0 en nuestro caso, así que, no hay diferencia. Pueden jugar con este y ver qué diferencia hace. La temperatura es un factor de escala utilizado para gestionar la nitidez de una distribución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestros logits (outputs), necesitamos objetivos (targets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos lo que esperamos que aprenda este modelo: **Queremos que aprenda \"representaciones similares (vectores)\" para una imagen dada y el título que la describe. Es decir, si le damos una imagen o el texto que la describe, queremos que produzca los mismos 256 vectores para ambos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025366,
     "end_time": "2021-04-05T08:01:51.972338",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.946972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction=\"none\")\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction=\"none\")\n",
    "        loss = (images_loss + texts_loss) / 2.0  # shape: (batch_size)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(preds, targets, reduction=\"none\"):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que, en el mejor de los casos, las matrices text_embeddings e image_embedding deberían ser iguales porque describen cosas similares. Pensemos ahora: si esto ocurre, ¿cómo sería la matriz logits? Veámoslo con un ejemplo sencillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple Example\n",
    "batch_size = 4\n",
    "dim = 256\n",
    "embeddings = torch.randn(batch_size, dim)\n",
    "out = embeddings @ embeddings.T\n",
    "print(F.softmax(out, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que logits, en el mejor de los casos, será una matriz que si tomamos su softmax, tendrá 1.0s en la diagonal (¡Una matriz identidad por llamarla con palabras elegantes!). Como el trabajo de la función de pérdida es hacer que las predicciones del modelo sean similares a los objetivos (¡al menos en la mayoría de los casos!), queremos una matriz así como objetivo. Esa es la razón por la que calculamos las matrices images_similarity y texts_similarity en el bloque de código anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestra matriz de objetivos, utilizaremos la entropía cruzada simple para calcular la pérdida real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE**: Admitiremos que hay una forma más sencilla de calcular esta pérdida en PyTorch; haciendo esto: nn.CrossEntropyLoss()(logits, torch.arange(batch_size)). ¿Por qué no lo utilizamos aquí? Por 2 razones: \n",
    "\n",
    "1) El conjunto de datos que estamos utilizando tiene múltiples leyendas para una sola imagen; por lo tanto, existe la posibilidad de que dos imágenes idénticas con sus leyendas similares existan en un lote (es raro, pero puede suceder). Tomar la pérdida con este método más sencillo ignorará esta posibilidad y el modelo aprenderá a separar dos representaciones (suponerlas diferentes) que en realidad son iguales. Obviamente, no queremos que esto suceda por lo que calculamos toda la matriz objetivo de una manera que se ocupa de estos casos de borde. \n",
    "\n",
    "2) Haciéndolo de la manera que en que lo hicimos, nos da mejor comprensión de lo que está sucediendo en esta función de pérdida, por lo que, es más intuitivo para su comprensión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013097,
     "end_time": "2021-04-05T08:01:51.998635",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.985538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041512,
     "end_time": "2021-04-05T08:01:52.054352",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.012840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n",
    "    max_id = dataframe.index.max() + 1 if not CFG.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe.index.isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe.index.isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe\n",
    "\n",
    "\n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041512,
     "end_time": "2021-04-05T08:01:52.054352",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.012840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    scaler = GradScaler()\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        with autocast():\n",
    "            loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        with autocast():\n",
    "            loss = model(batch)\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_df, valid_df = make_train_valid_dfs()\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "\n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    params = [\n",
    "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
    "        {\n",
    "            \"params\": itertools.chain(\n",
    "                model.image_projection.parameters(), model.text_projection.parameters()\n",
    "            ),\n",
    "            \"lr\": CFG.head_lr,\n",
    "            \"weight_decay\": CFG.weight_decay,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(params, weight_decay=0.0)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
    "    )\n",
    "    step = \"epoch\"\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = valid_epoch(model, valid_loader)\n",
    "\n",
    "        if valid_loss.avg < best_loss:\n",
    "            best_loss = valid_loss.avg\n",
    "            torch.save(model.state_dict(), \"best.pt\")\n",
    "            print(\"Saved Best Model!\")\n",
    "\n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar la siguiente celda empezar a entrenar el modelo. Cada epoch debería tardar menos minutos en la GPU (¡incluso una epoch es suficiente!). Puede llegar a tardar un minuto antes de que empiece el entrenamiento porque vamos a codificar todos los subtítulos una vez en el conjunto de datos válido y de entrenamiento, ¡así que por favor no lo detengas, todo funciona correctamente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "papermill": {
     "duration": 16449.265031,
     "end_time": "2021-04-05T12:36:01.333760",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.068729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 56/4046 [00:27<31:23,  2.12it/s, lr=0.0001, train_loss=nan]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos terminado de entrenar el modelo. Ahora tenemos que hacer la inferencia, que en nuestro caso consistirá en dar al modelo un texto y pedirle que recupere las imágenes más relevantes de un conjunto de validación (o prueba) que no se ha visto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta función, cargamos el modelo que hemos guardado tras el entrenamiento, lo alimentamos con imágenes del conjunto de validación y devolvemos image_embeddings con forma (valid_set_size, 256) y el propio modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(valid_df, model_path):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "\n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
    "    model.eval()\n",
    "\n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return model, torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, valid_df = make_train_valid_dfs()\n",
    "model, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función realiza la tarea final que deseábamos que nuestro modelo fuera capaz de hacer: obtiene el modelo, image_embeddings y una consulta de texto. Mostrará las imágenes más relevantes del conjunto de validación. Veamos cómo funciona después de todo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.025647,
     "end_time": "2021-04-05T12:36:01.385717",
     "exception": false,
     "start_time": "2021-04-05T12:36:01.360070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(CFG.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "\n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "\n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
    "\n",
    "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    for match, ax in zip(matches, axes.flatten()):\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{match}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_matches(\n",
    "    model,\n",
    "    image_embeddings,\n",
    "    query=\"young people dancing in a party\",\n",
    "    image_filenames=valid_df[\"image\"].values,\n",
    "    n=9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fundamentos de CLIP**\n",
    "\n",
    "* ¿Qué es CLIP y para qué sirve?\n",
    "* ¿Cómo se entrena CLIP y qué hace único a su enfoque de entrenamiento?\n",
    "* Explica el concepto de \"aprendizaje contrastivo\" en el contexto de CLIP.\n",
    "* ¿Cuáles son las principales diferencias entre CLIP y otros modelos de visión por computadora o procesamiento de lenguaje natural?\n",
    "\n",
    "**Aspectos Técnicos**\n",
    "\n",
    "* Describe cómo CLIP procesa las imágenes y los textos para realizar predicciones.\n",
    "* ¿Cómo contribuye el entrenamiento con un conjunto de datos diverso al rendimiento de CLIP en diferentes tareas?\n",
    "* Explica cómo CLIP logra ser eficaz en tareas de clasificación de imágenes sin necesidad de entrenamiento específico para esas tareas.\n",
    "\n",
    "**Aplicaciones y Casos de Uso**\n",
    "\n",
    "* Proporciona ejemplos de aplicaciones prácticas donde CLIP podría ser particularmente útil.\n",
    "* ¿Cómo puede CLIP ser utilizado para mejorar los sistemas de búsqueda visual? Proporciona un ejemplo específico.\n",
    "* ¿Cuáles son las implicaciones éticas y los desafíos potenciales al utilizar modelos como CLIP en aplicaciones del mundo real?\n",
    "\n",
    "**Comparaciones y Contrastes**\n",
    "\n",
    "* Compara CLIP con modelos tradicionales de visión por computadora en términos de flexibilidad y generalización.\n",
    "* ¿En qué se diferencia CLIP de otros modelos de aprendizaje multimodal?\n",
    "\n",
    "**Reflexión Crítica**\n",
    "\n",
    "* ¿Cuáles son las limitaciones de CLIP y cómo podrían abordarse en futuras versiones o modelos similares?\n",
    "* Discute el impacto potencial de la implementación de CLIP en la privacidad y la seguridad de la información.\n",
    "* Reflexiona sobre cómo el desarrollo de tecnologías como CLIP podría influir en el futuro del procesamiento de lenguaje natural y la visión por computadora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/dance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias:\n",
    "\n",
    "* The Annotated ResNet-50: https://towardsdatascience.com/the-annotated-resnet-50-a6c536034758\n",
    "* HuggingFace DistilBERT: https://www.scaler.com/topics/nlp/distilbert/\n",
    "* Learning Transferable Visual Models From Natural Language Supervision: https://arxiv.org/abs/2103.00020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptado de:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/OpenAI%20CLIP%20Simple%20Implementation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lenguaje Matemático](./Images/Matematicas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Contacto](./Images/Contacto.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
